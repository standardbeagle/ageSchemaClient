# Product Requirements Document: Apache AGE Batch Loader

## Overview
The Apache AGE Batch Loader is a specialized component of the ageSchemaClient library designed to efficiently load large volumes of graph data into Apache AGE graph databases. This PRD outlines the requirements for a new implementation that uses a temporary table approach to efficiently load data with minimal transactions.

## Background
Apache AGE is a PostgreSQL extension that provides graph database functionality. The ageSchemaClient library provides a TypeScript interface for working with Apache AGE. The current batch loader implementation creates many small transactions, which is inefficient and error-prone for large datasets.

## Goals
- Create a new batch loader implementation that uses the temporary table approach for efficient data loading
- Reduce the number of transactions to improve performance and reliability
- Improve performance by minimizing network traffic and round trips between client and database
- Ensure compatibility with the existing connection pool and parameter handling mechanisms
- Provide comprehensive error handling and reporting
- Maintain the same API interface for backward compatibility
- Secure parameter passing using sql parameters

## Non-Goals
- Supporting databases other than PostgreSQL with Apache AGE
- Supporting graph databases other than Apache AGE
- Changing the schema validation logic
- Any solution involving string interpolation for parameter passing

## Requirements

### Functional Requirements

#### 1. Temporary Table Approach
- Use the existing age_params temporary table for storing data
- Create PostgreSQL functions that return ag_catalog.agtype for use with UNWIND in Cypher
- Support parameterized queries for all data loading operations
- Ensure all function parameters and outputs are ag_catalog.agtype for Cypher compatibility

#### 2. Transaction Management
- Minimize the number of transactions for improved performance
- Ensure proper transaction management with commit and rollback support
- Use the QueryBuilder's setParam method to store data in the age_params table

#### 3. Connection Pool Integration
- Use the existing connection pool for all database operations
- Ensure compatibility with the existing age_params temporary table
- Verify that AGE is loaded and ag_catalog is in the search_path before proceeding
- Return connections to the pool after use

#### 4. Data Loading
- Support loading vertices of multiple types
- Support loading edges of multiple types
- Support properties on vertices and edges
- Support batch loading to handle large datasets efficiently
- Validate data against the schema before loading

#### 5. Error Handling
- Provide detailed error messages for all failure scenarios
- Roll back transactions on error
- Clean up temporary resources on success or failure
- Report validation errors before attempting to load data

#### 6. Progress Reporting
- Report progress during the loading process
- Provide statistics on the number of vertices and edges loaded
- Report any warnings or errors encountered during loading

### Technical Requirements

#### 1. Database Schema
- Use the existing age_params temporary table for storing data
- Create PostgreSQL functions that return ag_catalog.agtype for use with UNWIND in Cypher
- Ensure all functions are schema-driven but as generic as possible

#### 2. API Design
- Maintain the same API interface as the existing batch loader
- Support the same options and configuration parameters
- Provide backward compatibility for existing code

#### 3. Performance
- Minimize network traffic between client and database
- Reduce the number of round trips between client and database
- Support batch processing for large datasets
- Optimize SQL queries for performance

#### 4. Security
- Use parameterized queries to prevent SQL injection
- Validate input data before loading
- Ensure proper error handling to prevent information leakage

## Success Criteria
- The new batch loader successfully loads all test data
- The new batch loader performs at least 2x faster than the existing implementation
- All integration tests pass with the new implementation
- The new implementation properly handles errors and cleans up resources

## Implementation Plan

### Phase 1: Design and Prototyping
- Design the PostgreSQL functions for retrieving vertex and edge data
- Design the Cypher queries for creating vertices and edges
- Create a prototype implementation

### Phase 2: Implementation
- Implement the PostgreSQL functions
- Implement the client-side code for the new batch loader
- Implement error handling and progress reporting

### Phase 3: Testing
- Create unit tests for the new batch loader
- Create integration tests for the new batch loader
- Test with large datasets to verify performance
- Test error handling and recovery

### Phase 4: Documentation and Deployment
- Document the new batch loader implementation
- Update the API documentation
- Create examples for using the new batch loader
- Deploy the new batch loader as part of the ageSchemaClient library

## Timeline
- Phase 1: 1 week
- Phase 2: 2 weeks
- Phase 3: 1 week
- Phase 4: 1 week
- Total: 5 weeks

## Risks and Mitigations
- Risk: Apache AGE compatibility issues
  - Mitigation: Verify compatibility with the target Apache AGE version before implementation
- Risk: Performance issues with large datasets
  - Mitigation: Test with progressively larger datasets during development
- Risk: Transaction management issues
  - Mitigation: Implement comprehensive error handling and recovery
- Risk: SQL injection vulnerabilities
  - Mitigation: Use parameterized queries for all data loading operations
- Risk: Temporary resource leaks
  - Mitigation: Implement proper cleanup in finally blocks

## Dependencies
- Apache AGE extension must be installed and configured
- PostgreSQL must support the required function functionality
- The existing connection pool must be properly configured
- The existing schema validation logic must be working correctly

## Appendix

### Example Usage
```typescript
// Create a schema loader with the new batch loader
const schemaLoader = new SchemaLoader(schema, queryExecutor, {
  validateBeforeLoad: true,
  defaultGraphName: 'my_graph',
  defaultBatchSize: 1000
});

// Load data using the new batch loader
const result = await schemaLoader.loadGraphData({
  vertices: {
    Person: [
      { id: '1', name: 'Alice', age: 30 },
      { id: '2', name: 'Bob', age: 25 }
    ],
    Company: [
      { id: '3', name: 'Acme Inc.', founded: 1990 }
    ]
  },
  edges: {
    WORKS_AT: [
      { from: '1', to: '3', since: 2015, position: 'Manager' },
      { from: '2', to: '3', since: 2018, position: 'Developer' }
    ],
    KNOWS: [
      { from: '1', to: '2', since: 2010 }
    ]
  }
});

console.log(`Loaded ${result.vertexCount} vertices and ${result.edgeCount} edges`);
```

### Implementation Details

#### 1. Data Storage in age_params Table
```typescript
// Store vertex data in age_params table
for (const [vertexType, vertices] of Object.entries(graphData.vertices)) {
  await queryBuilder.setParam(`vertex_${vertexType}`, vertices);
}

// Store edge data in age_params table
for (const [edgeType, edges] of Object.entries(graphData.edges)) {
  await queryBuilder.setParam(`edge_${edgeType}`, edges);
}
```

#### 2. PostgreSQL Functions for Data Retrieval
```sql
-- Function to retrieve vertices by type
CREATE OR REPLACE FUNCTION schema_name.get_vertices(vertex_type ag_catalog.agtype)
RETURNS ag_catalog.agtype AS $$
DECLARE
  vertex_type_text TEXT;
  result_array ag_catalog.agtype;
BEGIN
  -- Extract the text value from the agtype parameter
  SELECT vertex_type::text INTO vertex_type_text;
  -- Remove quotes if present
  vertex_type_text := REPLACE(REPLACE(vertex_type_text, '"', ''), '''', '');

  -- Get the data for the specified vertex type and convert to ag_catalog.agtype
  SELECT value::text::ag_catalog.agtype
  INTO result_array
  FROM age_params
  WHERE key = 'vertex_' || vertex_type_text;

  RETURN result_array;
END;
$$ LANGUAGE plpgsql;

-- Function to retrieve edges by type
CREATE OR REPLACE FUNCTION schema_name.get_edges(edge_type ag_catalog.agtype)
RETURNS ag_catalog.agtype AS $$
DECLARE
  edge_type_text TEXT;
  result_array ag_catalog.agtype;
BEGIN
  -- Extract the text value from the agtype parameter
  SELECT edge_type::text INTO edge_type_text;
  -- Remove quotes if present
  edge_type_text := REPLACE(REPLACE(edge_type_text, '"', ''), '''', '');

  -- Get the data for the specified edge type and convert to ag_catalog.agtype
  SELECT value::text::ag_catalog.agtype
  INTO result_array
  FROM age_params
  WHERE key = 'edge_' || edge_type_text;

  RETURN result_array;
END;
$$ LANGUAGE plpgsql;
```

#### 3. Cypher Queries for Creating Vertices
```cypher
-- Create vertices using UNWIND with the get_vertices function
UNWIND schema_name.get_vertices($vertex_type) AS vertex_data
CREATE (v:$vertex_type {
  id: vertex_data.id,
  -- Add other properties dynamically based on vertex_data
  ...vertex_data
})
RETURN count(v) AS created_vertices
```

#### 4. Cypher Queries for Creating Edges
```cypher
-- Create edges using UNWIND with the get_edges function
UNWIND schema_name.get_edges($edge_type) AS edge_data
MATCH (from {id: edge_data.from})
MATCH (to {id: edge_data.to})
CREATE (from)-[:$edge_type {
  -- Add edge properties dynamically based on edge_data
  ...edge_data
}]->(to)
RETURN count(*) AS created_edges
```

#### 5. Complete Loading Process
```typescript
// Example implementation of the loadGraphData method
async loadGraphData(graphData: {
  vertices: Record<string, any[]>,
  edges: Record<string, any[]>
}, options: LoadOptions = {}): Promise<LoadResult> {
  const graphName = options.graphName || this.defaultGraphName;
  const queryBuilder = new QueryBuilder(this.schema, this.queryExecutor, graphName);

  try {
    // 1. Store vertex data in age_params table
    for (const [vertexType, vertices] of Object.entries(graphData.vertices)) {
      await queryBuilder.setParam(`vertex_${vertexType}`, vertices);
    }

    // 2. Create vertices for each type
    const vertexResults = [];
    for (const vertexType of Object.keys(graphData.vertices)) {
      const result = await this.queryExecutor.executeCypher(`
        UNWIND schema_name.get_vertices($vertex_type) AS vertex_data
        CREATE (v:${vertexType} {
          id: vertex_data.id,
          name: vertex_data.name,
          -- Add other properties as needed
          ...
        })
        RETURN count(v) AS created_vertices
      `, { vertex_type: vertexType }, graphName);

      vertexResults.push(result);
    }

    // 3. Store edge data in age_params table
    for (const [edgeType, edges] of Object.entries(graphData.edges)) {
      await queryBuilder.setParam(`edge_${edgeType}`, edges);
    }

    // 4. Create edges for each type
    const edgeResults = [];
    for (const edgeType of Object.keys(graphData.edges)) {
      const result = await this.queryExecutor.executeCypher(`
        UNWIND schema_name.get_edges($edge_type) AS edge_data
        MATCH (from {id: edge_data.from})
        MATCH (to {id: edge_data.to})
        CREATE (from)-[:${edgeType} {
          -- Add edge properties as needed
          ...
        }]->(to)
        RETURN count(*) AS created_edges
      `, { edge_type: edgeType }, graphName);

      edgeResults.push(result);
    }

    // 5. Calculate total counts
    const vertexCount = vertexResults.reduce((total, result) =>
      total + parseInt(result.rows[0].created_vertices, 10), 0);

    const edgeCount = edgeResults.reduce((total, result) =>
      total + parseInt(result.rows[0].created_edges, 10), 0);

    return { vertexCount, edgeCount };
  } finally {
    // Clean up by resetting the query builder
    queryBuilder.reset();
  }
}
```
